\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{neuralnetwork}
\usepackage{sidecap}
\usepackage{hyperref}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\newcommand{\setreal}{\mathbb{R}}

\title{Idea: Adaption of theory to networks with biases}
\author{Leander Kurscheidt}

\begin{document}
\maketitle
\begin{abstract}
\end{abstract}
\section{Affine Transformations}

Since our NN "somewhat" resembles linear functions following from the rule $\sigma(x)=\sigma'(x)x$ (i see this rule as separating the linear from the non-linear part),
i am curious to see whether ideas of affine spaces and how to integrate them into linear-spaces translate.

I think they do.

\section{Idea}

Affine subspaces are usually define via $A=v + U_V$, where $U_V$ is a subspace of $V$ and $v$ is a vector of $V$ (see \href{https://de.wikipedia.org/wiki/Affiner_Unterraum}{(german) wikipedia}). But you can work in them using \href{https://en.wikipedia.org/wiki/Homogeneous_coordinates}{homogenous coordinates}.

Every affine transformation can be turned into a linear transformation with a constant 1-input using the \href{https://en.wikipedia.org/wiki/Augmented_matrix}{Augmented Matrix} trick.\\
For example the intercept in linear models in statistics is often modeled this way. \\
We can adapt this trick with only one additional constant input using this matrix:
\[
W'=\begin{bmatrix}
    w_{11}       & w_{12} & w_{13} & \dots & w_{1n} & bias_1\\
    w_{21}       & w_{22} & w_{23} & \dots & w_{2n} & bias_2\\
    \hdotsfor{6} \\
	w_{d1}       & w_{d2} & w_{d3} & \dots & w_{dn} & bias_n\\
	0 & \hdotsfor{3} & 0 & 1
\end{bmatrix}
= 
\begin{bmatrix}
    W & & & b\\
	0 & \dots & 0 & 1
\end{bmatrix}
\]\\
the last line is the difference to the usual construction and should be ommitted in the last layer.\\

This results in:$f_\theta(x)=f_{\theta'}()$,\\
where $\theta'$ is the parameter adpated in the above schema.\\

Usually proofs get a lot simple when you just have to wory about keeping one input constant. I think a lot of the proofs from the paper should still hold, for example Leamma 2.1 doesn't change.\\

For this to work, we need to add the additional requirement that $\sigma(1)=1$
\pagebreak
\section{Proof}

In the following section, neural networks with biases as defined as:\\
\begin{align*}
f_{\theta}(x)=\sigma_{L+1}(\sigma_L(\dots \sigma_2(\sigma_1(xW^0 + b^0)W^1 + b^1)W^2 +b^2)\dots W^L + b^L)
\end{align*}
We call $b^i$ bias and additionally need the follwing constraint on the activation function $\sigma_i$:\\
\begin{align*}
\forall i \in {L+1, \dots, 1}. \exists c_i. \sigma_i(c_i)=1
\end{align*}
For $f'_{\theta'}$ with $\theta' = (W'^0, W'^1,\dots, W'^L)$ we define:
\begin{flalign*}
            &f'_{\theta'^0}(z) := \sigma_1(zW'^0)\text{, where } \theta'^0 := (W'^0) \\
            &f'_{\theta'^L} \text{, so that}\\
            &f'_{\theta'} = f'_{\theta'^L} \circ f'_{\theta'^0} 
            \text{ and }\\
            &\theta'^L = (W'^1,\dots, W'^L)
\end{flalign*}

\theoremstyle{definition}
\begin{definition}{Homogenous Coordinates for Neural Networks}\\
Let $f_{\theta}$ be a neural network with biases. Then augumented neural network $f'_{\theta'}$ is a neural network as defined in Definition 1, (2.1) in [TODO: ref], where:\\
$\theta' = (W'^0, \dots, W'^L)$\\
and\\
\[
W'^i=\begin{bmatrix}
    w_{11}^i       & w_{12}^i & w_{13}^i & \dots & w_{1k_i}^i & b_1^i\\
    w_{21}^i       & w_{22}^i & w_{23}^i & \dots & w_{2k_i}^i & b_2^i\\
    \hdotsfor{6} \\
	w_{k_{i+1}1}^i       & w_{k_{i+1}2}^i & w_{k_{i+1}3}^i & \dots & w_{k_{i+1}k_i}^i & b_{k_{i+1}}^i\\
	0 & \hdotsfor{3} & 0 & c_i
\end{bmatrix}
= 
\begin{bmatrix}
    W^i & & & b^i\\
	0 & \dots & 0 & c_i
\end{bmatrix} \in \mathbb{R}^{k_i + 1, k_{i + 1} + 1}  \forall i \in \{0, \dots (L-1)\}
\]\\
\[
W'^i=
\begin{bmatrix}
    W^i & b^i
\end{bmatrix} \in \mathbb{R}^{k_i + 1, K}  \text{ with } i = L
\]
\end{definition}

\begin{theorem}{Equality of the augumented NN}\\
For all NN with Bias $f_{\theta}$:\\
$f_{\theta}(x)=f'_{\theta'}(\begin{bmatrix}     x\\     1 \end{bmatrix})$
\end{theorem}

\begin{proof}
    via Induction over L.\\
    \begin{enumerate}
        \item For $L=0$:\\
        \begin{align}
            \Theta'&=(W'^0) \\
            W'^0 &= \begin{bmatrix}
                W^0 & b^0
            \end{bmatrix} \text{ because } 0=L \\
            f'_{\theta'}(\begin{bmatrix}     x\\     1 \end{bmatrix}) &= \sigma_1(\begin{bmatrix}     x\\     1 \end{bmatrix}W'^0) \\
            &= \sigma_1(\begin{bmatrix}     x\\     1 \end{bmatrix}\begin{bmatrix}
                W^0 & b^0
            \end{bmatrix})\\
            &= \sigma_1(xW^0 + b^0)\\
            &= f_{\theta}(x)
        \end{align}
        \item For $L \rightarrow (L+1)$ holds Theorem 3.1
        \item $L \rightarrow (L+1)$:\\
        \begin{align}
            \theta' &= (W'^0, W'^1,\dots, W'^L) \\
            W'^{0} &= \begin{bmatrix}
                W^0 & & & b^0\\
                0 & \dots & 0 & c_0
            \end{bmatrix} \text{ because } 0\neq(L+1)\\
            \\
            f'_{\theta'^L}(\begin{bmatrix}     x\\     1 \end{bmatrix}) &= \sigma_1(\begin{bmatrix}     x\\     1 \end{bmatrix}W'^0)\\
            &=\begin{bmatrix}
                \sigma_1(\begin{bmatrix}     x\\     1 \end{bmatrix}\begin{bmatrix}
                    W^0 & & & b^0\\
                \end{bmatrix} + 1*b^0)\\
                \sigma_1(1*c_0)
            \end{bmatrix}\\
            &=\begin{bmatrix}
                f_{\theta^0}(x)\\
                1
            \end{bmatrix}\\
            f'_{\theta'}(\begin{bmatrix}     x\\     1 \end{bmatrix}) &=(f'_{\theta'^L} \circ f'_{\theta'^0}) (\begin{bmatrix}     x\\     1 \end{bmatrix})\\
            &= f'_{\theta'^L}(\begin{bmatrix}
                f_{\theta^0}(x)\\
                1
            \end{bmatrix})\\
            &= (f_{\theta^L} \circ f_{\theta^0})(x)\\
            &= f'_{\theta}(x)
        \end{align}
    \end{enumerate}
\end{proof}


\bibliographystyle{alpha}
\bibliography{sample}

\end{document}